import torch
import torch.nn as nn 
import torch.nn.functional as F
from itertools import product
import numpy as np
import re


class Loss(object):
    def __init__(self, name):
        super().__init__()
        if name == "euclidean_distance":
            self._loss_ = self._euclidean_distance
        elif name == 'kl_diversity':
            self._loss_ = self._kl_diversity
        else:
            raise Exception(f'Loss({name}) is not implemented.')
    
    def __call__(self, target, predictions):
        return self._loss_(target, predictions)
    
    def _euclidean_distance(self, originals, predicts):
        """ Compute the Euclidean distance
            between original node features and 
            the predicted node features generated by mixing matrixs and sources
        
        Parameters
        ----------
        originals : torch.tensor
            (batch_size, max_num_nodes_in_path, feature_dim)
            Original node features looked up from nn.Embedding
        
        predicts : torch.tensor
            (batch_size, max_num_nodes_in_path, feature_dim)
            Node features computed by the mixing matrixs and sources
        
        Return
        ----------
        Loss -> Euclidean distance
        """
        # torch.cdist -> (x1, x2, p=2.0, compute_mode='use_mm_for_euclid_dist_if_necessary')
        # Computes batched the p-norm distance between each pair of the two collections of row vectors.
        #
        # Compute euclidean distance
        return F.pairwise_distance(predicts, originals).mean()
    
    def _kl_diversity(self, original_dists, predict_dists):
        return F.kl_div(predict_dists, original_dists)




    def nll_loss(self, target, output):
        return F.nll_loss(output, target) 

    def square_exp_loss(self, target, output, beta=1.0):
        """
        output: a (batch_size, 1) tensor, value should be positive
        target: a (batch_size, ) tensor of dtype int
        beta: a float weight of negative samples
        """
        loss = (output[target==1]**2).sum() + beta * torch.exp(-1.0*output[target==0]).sum()
        return loss

    def bce_loss(self, target, output, beta=1.0):
        """
        output: a (batch_size, 1) tensor
        target: a (batch_size, ) tensor of dtype int
        
        Note: here we revert the `target` because `output` is the "energy score" and thus smaller value indicates it is more likely to be a true position 
        """
        loss = F.binary_cross_entropy_with_logits(output.squeeze(), 1.0-target.float(), reduction="sum")
        return loss

    def margin_rank_loss(self, target, output, margin=1.0):
        label = target.cpu().numpy()
        sep_01 = np.array([0, 1], dtype=label.dtype)
        sep_10 = np.array([1, 0], dtype=label.dtype)

        # fast way to find subarray indices in a large array, c.f. https://stackoverflow.com/questions/14890216/return-the-indexes-of-a-sub-array-in-an-array
        sep10_indices = [(m.start() // label.itemsize)+1 for m in re.finditer(sep_10.tostring(), label.tostring())]
        end_indices = [(m.start() // label.itemsize)+1 for m in re.finditer(sep_01.tostring(), label.tostring())]
        end_indices.append(len(label))
        start_indices = [0] + end_indices[:-1]

        pair_indices = []
        for start, middle, end in zip(start_indices, sep10_indices, end_indices):
            pair_indices.extend(list(product(range(start, middle), range(middle, end))))
        positive_indices = [ele[0] for ele in pair_indices]
        negative_indices = [ele[1] for ele in pair_indices]

        y = -1 * torch.ones(output[positive_indices,:].shape[0]).to(target.device)
        loss = F.margin_ranking_loss(output[positive_indices,:], output[negative_indices,:], y, margin=margin, reduction="sum")
        return loss

    def info_nce_loss(self, target, output):
        """
        output: a (batch_size, 1+negative_size) tensor
        target: a (batch_size, ) tensor of dtype long, all zeros
        """
        return F.cross_entropy(output, target, reduction="sum")
